---
title: "Precisão global kappa"
lang: pt
execute:
  freeze: auto  # never re-render during project render #  freeze: auto  # re-render only when source changes
  cache: true 
---

A precisão global e o índice kappa são métricas comuns usadas para avaliar a precisão de modelos de classificação em R. O índice kappa leva em consideração a concordância observada entre as previsões do modelo e as classes reais, ajustando-a para a concordância esperada ao acaso. Ele fornece uma medida da precisão do modelo que leva em consideração a chance de acerto aleatório.

## Carregando dataset

```{r}
datasetfull<-readRDS("../rds/datasetfull_regras.rds")

novo_dataset <- scale(datasetfull[, c("declividade_valor"
                                  ,"def_hid_valor"
                                  ,"dist_sede_municipal_valor"
                                  ,"dist_vias_municipais_valor"
                                  ,"drenagem_cd_aptidao"
                                  ,"erodibilidade_cd_aptidao"
                                  ,"pluviosidade_valor"
                                  ,"temperatura_valor"
                                  ,"textura_cd_aptidao"
                                  ,"umidade_valor"
                                  ,"uso_cd_aptidao"
                                  ,"aptidao"
                                  )])

```

## Carregando libs

```{r}
#| warning: false
if(!require('caret')) {install.packages('caret', repos='http://cran-r.c3sl.ufpr.br/')}
library(caret)
if(!require('rpart')) {install.packages('rpart', repos='http://cran-r.c3sl.ufpr.br/')}
library(rpart)
if(!require('knitr')) {install.packages('knitr', repos='http://cran-r.c3sl.ufpr.br/')}
library(knitr)
```

## Amostra dos dados

```{r}
#| label: tbl-example-datasetfull
#| tbl-cap: "Exemplos dos dados"

library(knitr)
kable(head(datasetfull))
```

## Selecionando dados para treino e teste
```{r}
# Selecionando os dados classificados
conjunto_de_dados <- subset(datasetfull, aptidao != 0)

# Defina a proporção desejada para treinamento (70%) e teste (30%)
proporcao_treinamento <- 0.7

# Calcule o número de observações para treinamento
tamanho_treinamento <- round(nrow(conjunto_de_dados) * proporcao_treinamento)

# Crie um vetor de índices aleatórios para as observações
indices_aleatorios <- sample(1:nrow(conjunto_de_dados))

# Selecione as observações de treinamento com base nos índices aleatórios
conjunto_treinamento <- conjunto_de_dados[indices_aleatorios[1:tamanho_treinamento], ]

# Selecione as observações de teste com base nos índices aleatórios restantes
conjunto_teste <- conjunto_de_dados[indices_aleatorios[(tamanho_treinamento + 1):nrow(conjunto_de_dados)], ]
if(!file.exists("../rds/modelo_arvore.rds")){
    modelo_arvore <- rpart(aptidao ~ ., data = conjunto_treinamento, method = "class")
    saveRDS(modelo_arvore, "../rds/modelo_arvore.rds")
}else{
    modelo_arvore <- readRDS("../rds/modelo_arvore.rds")
}

previsoes <- predict(modelo_arvore, newdata = conjunto_teste, type = "class")

```

## Calculando matriz de confusão
```{r}
#| label: tbl-matriz-confusao
#| tbl-cap: "Matriz de confusão"
conf_matrix <- confusionMatrix(previsoes, factor(conjunto_teste$aptidao))

kable(conf_matrix$table)
```

## Obtenha a precisão global

```{r}
accuracy <- conf_matrix$overall["Accuracy"]
cat("Precisão Global:", accuracy, "\n")
```

## Obtenha o índice kappa

```{r}
kappa <- conf_matrix$overall["Kappa"]
cat("Índice Kappa:", kappa, "\n")
```
